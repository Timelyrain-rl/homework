{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784aacf1-0e7b-4dc2-b195-1d54aaead515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "968b75a7-57c9-4299-83ea-0cb0d57f7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_folder(folder_path):\n",
    "    \"\"\"\n",
    "    读取文件夹下所有文件\n",
    "\n",
    "    :param folder_path:存放txt文件的文件夹路径，这里就是同目录下的 text_train 文件夹\n",
    "    :return:存放文件内容的列表 tokens\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "\n",
    "    for text_name in os.listdir(folder_path):\n",
    "        # print(text_name)\n",
    "        file_path = os.path.join(folder_path, text_name)\n",
    "        if os.path.isfile(file_path) and text_name.endswith('.txt'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read().replace(\"\\n\",\"\")\n",
    "                tokens.append(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f2e916-3063-4c59-a10a-6c8f58c3bdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_del(tokens,stopwords_path):\n",
    "    \"\"\"\n",
    "    删除停用词\n",
    "    :param \n",
    "        stopwords_path 停用词路径\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    def stopwords_load(stopwords_path):\n",
    "        \"\"\"\n",
    "        加载停用词\n",
    "        :param stopwords_path 停用词路径\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        stopwords=set()\n",
    "        with open(stopwords_path,'r',encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            stopwords.add(line.strip())\n",
    "        return stopwords\n",
    "        \n",
    "    corpus = []\n",
    "    stopwords = stopwords_load(stopwords_path)\n",
    "    jieba.load_userdict('self_userdict.txt')\n",
    "    stopwords.add(' ')\n",
    "    stopwords.add('/')\n",
    "    for token in tokens:\n",
    "        token_cutted = jieba.lcut(token)\n",
    "        for word in token_cutted:\n",
    "            if word not in stopwords:\n",
    "                corpus.append(word)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e40724cf-b935-4192-b29a-2814bf9f66a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = read_folder(\"text_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41e7288c-5ad7-4b1e-b87a-c1189da09013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\47226\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.442 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "corpus = stopwords_del(tokens,\"stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d821d8f9-ef8a-4679-8618-c084865decc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are totoally 37571 different words in the corpus\n",
      "word 发展, its id 0, its word freq 7944\n",
      "word 建设, its id 1, its word freq 4917\n",
      "word 中国, its id 2, its word freq 4727\n",
      "word 年, its id 3, its word freq 4258\n",
      "word 国家, its id 4, its word freq 4178\n",
      "word 工作, its id 5, its word freq 3747\n",
      "word 新, its id 6, its word freq 3447\n",
      "word 经济, its id 7, its word freq 3323\n",
      "word 中, its id 8, its word freq 3243\n",
      "word 社会主义, its id 9, its word freq 2800\n",
      "word 人民, its id 10, its word freq 2798\n",
      "word 推进, its id 11, its word freq 2794\n",
      "word 上, its id 12, its word freq 2318\n",
      "word 改革, its id 13, its word freq 2292\n",
      "word 社会, its id 14, its word freq 2270\n",
      "word 制度, its id 15, its word freq 2033\n",
      "word 企业, its id 16, its word freq 1981\n",
      "word 地方, its id 17, its word freq 1838\n",
      "word 历史, its id 18, its word freq 1830\n",
      "word 我国, its id 19, its word freq 1813\n",
      "word 增长, its id 20, its word freq 1772\n",
      "word 月, its id 21, its word freq 1764\n",
      "word 政策, its id 22, its word freq 1745\n",
      "word 文化, its id 23, its word freq 1737\n",
      "word 推动, its id 24, its word freq 1681\n",
      "word 全国, its id 25, its word freq 1680\n",
      "word 提高, its id 26, its word freq 1657\n",
      "word 实施, its id 27, its word freq 1633\n",
      "word 政府, its id 28, its word freq 1602\n",
      "word 研究, its id 29, its word freq 1573\n",
      "word 生产, its id 30, its word freq 1546\n",
      "word 不, its id 31, its word freq 1540\n",
      "word 亿元, its id 32, its word freq 1524\n",
      "word 支持, its id 33, its word freq 1504\n",
      "word 农业, its id 34, its word freq 1481\n",
      "word 完善, its id 35, its word freq 1451\n",
      "word 加快, its id 36, its word freq 1417\n",
      "word 管理, its id 37, its word freq 1380\n",
      "word 体系, its id 38, its word freq 1377\n",
      "word 领导, its id 39, its word freq 1368\n",
      "word 创新, its id 40, its word freq 1364\n",
      "word 现代化, its id 41, its word freq 1297\n",
      "word 农村, its id 42, its word freq 1259\n",
      "word 中央, its id 43, its word freq 1242\n",
      "word 都, its id 44, its word freq 1233\n",
      "word 政治, its id 45, its word freq 1177\n",
      "word 增加, its id 46, its word freq 1171\n",
      "word 保障, its id 47, its word freq 1153\n",
      "word 地区, its id 48, its word freq 1133\n",
      "word 民族, its id 49, its word freq 1133\n"
     ]
    }
   ],
   "source": [
    "def build_dict(corpus):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param \n",
    "    :return:\n",
    "    \"\"\"\n",
    "    word_freq_dict = dict()\n",
    "    for word in corpus:\n",
    "        if word not in word_freq_dict:\n",
    "            word_freq_dict[word] = 0\n",
    "        word_freq_dict[word] += 1\n",
    "\n",
    "    word_freq_dict = sorted(word_freq_dict.items(), key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "    word2id_dict = dict()\n",
    "    word2id_freq = dict()\n",
    "    id2word_dict = dict()\n",
    "\n",
    "    for word, freq in word_freq_dict:\n",
    "        curr_id = len(word2id_dict)\n",
    "        word2id_dict[word] = curr_id\n",
    "        word2id_freq[word2id_dict[word]] = freq\n",
    "        id2word_dict[curr_id] = word\n",
    "\n",
    "    return word2id_freq, word2id_dict, id2word_dict\n",
    "\n",
    "word2id_freq, word2id_dict, id2word_dict = build_dict(corpus)\n",
    "vocab_size = len(word2id_freq)\n",
    "print(\"there are totoally %d different words in the corpus\" % vocab_size)\n",
    "for _, (word, word_id) in zip(range(50), word2id_dict.items()):\n",
    "    print(\"word %s, its id %d, its word freq %d\" % (word, word_id, word2id_freq[word_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70ab4ca4-2b69-459e-adbb-c84c407a3f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599960 tokens in the corpus\n",
      "[14, 477, 5, 1536, 14, 160, 8821, 152, 11, 514, 1, 105, 136, 100, 6, 2, 124, 94, 344, 1833, 334, 4, 2004, 2161, 13110, 16060, 8, 6, 2, 99, 16061, 300, 2666, 926, 1733, 2666, 1677, 334, 4, 1733, 2666, 402, 1615, 55, 12, 993, 500, 1924, 1259, 6814]\n"
     ]
    }
   ],
   "source": [
    "def convert_corpus_to_id(corpus, word2id_dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param \n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corpus = [word2id_dict[word] for word in corpus]\n",
    "    return corpus\n",
    "\n",
    "corpus = convert_corpus_to_id(corpus, word2id_dict)\n",
    "print(\"%d tokens in the corpus\" % len(corpus))\n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d020468-c9db-42b6-80cc-9430aabf55a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363428 tokens in the corpus\n",
      "[1536, 8821, 152, 514, 1833, 4, 2004, 2161, 13110, 16060, 16061, 2666, 926, 1733, 2666, 1677, 1733, 2666, 1615, 993, 500, 1924, 1259, 6814, 687, 2756, 7342, 21612, 86, 2429, 167, 292, 5080, 2241, 344, 2492, 3767, 1733, 2666, 11229, 559, 2826, 2114, 8822, 3896, 94, 344, 951, 1678, 3767]\n"
     ]
    }
   ],
   "source": [
    "#使用二次采样算法（subsampling）处理语料，强化训练效果\n",
    "def subsampling(corpus, word2id_freq):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param \n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #这个discard函数决定了一个词会不会被替换，这个函数是具有随机性的，每次调用结果不同\n",
    "    #如果一个词的频率很大，那么它被遗弃的概率就很大\n",
    "    def discard(word_id):\n",
    "        return random.uniform(0, 1) < 1 - math.sqrt(\n",
    "            1e-4 / word2id_freq[word_id] * len(corpus))\n",
    "\n",
    "    corpus = [word for word in corpus if not discard(word)]\n",
    "    return corpus\n",
    "\n",
    "corpus = subsampling(corpus, word2id_freq)\n",
    "print(\"%d tokens in the corpus\" % len(corpus))\n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17260650-6a77-4d98-aff7-866adc865e7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "center_word 资源整合, target 凝聚, label 1\n",
      "center_word 资源整合, target 剑, label 0\n",
      "center_word 资源整合, target 一齐, label 0\n",
      "center_word 资源整合, target 借机, label 0\n",
      "center_word 资源整合, target 相声, label 0\n",
      "center_word 方式, target 凝聚, label 1\n",
      "center_word 方式, target 改选, label 0\n",
      "center_word 方式, target 1022.85, label 0\n",
      "center_word 方式, target 发展观, label 0\n",
      "center_word 方式, target 之久, label 0\n",
      "center_word 党和国家, target 凝聚, label 1\n",
      "center_word 党和国家, target 史有, label 0\n",
      "center_word 党和国家, target 不行, label 0\n",
      "center_word 党和国家, target 附带, label 0\n",
      "center_word 党和国家, target 思之, label 0\n",
      "center_word 凝聚, target 党和国家, label 1\n",
      "center_word 凝聚, target 八纵, label 0\n",
      "center_word 凝聚, target 决不再, label 0\n",
      "center_word 凝聚, target 青, label 0\n",
      "center_word 凝聚, target 43.4, label 0\n",
      "center_word 资源整合, target 党和国家, label 1\n",
      "center_word 资源整合, target 跳板, label 0\n",
      "center_word 资源整合, target 敢干, label 0\n",
      "center_word 资源整合, target 缺一不可, label 0\n",
      "center_word 资源整合, target 获胜, label 0\n",
      "center_word 方式, target 党和国家, label 1\n",
      "center_word 方式, target 2.55%, label 0\n",
      "center_word 方式, target 不丹, label 0\n",
      "center_word 方式, target 概念, label 0\n",
      "center_word 方式, target 中国大百科全书出版社, label 0\n",
      "center_word 为首, target 党和国家, label 1\n",
      "center_word 为首, target 所急, label 0\n",
      "center_word 为首, target 怀念, label 0\n",
      "center_word 为首, target 渐趋, label 0\n",
      "center_word 为首, target 第二十四章, label 0\n",
      "center_word 国家, target 党和国家, label 1\n",
      "center_word 国家, target 执行力, label 0\n",
      "center_word 国家, target 一府, label 0\n",
      "center_word 国家, target 罚款, label 0\n",
      "center_word 国家, target 熊, label 0\n",
      "center_word 新生, target 党和国家, label 1\n",
      "center_word 新生, target 抢抓, label 0\n",
      "center_word 新生, target 汇成, label 0\n",
      "center_word 新生, target 盼, label 0\n",
      "center_word 新生, target 争取和平, label 0\n",
      "center_word 党和国家, target 新生, label 1\n",
      "center_word 党和国家, target 明码标价, label 0\n",
      "center_word 党和国家, target 四十七万, label 0\n",
      "center_word 党和国家, target 患儿, label 0\n",
      "center_word 党和国家, target 本条, label 0\n"
     ]
    }
   ],
   "source": [
    "def build_data(corpus, word2id_dict, word2id_freq, max_window_size = 3, negative_sample_num = 4):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param \n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    center_word_idx=0\n",
    "\n",
    "    while center_word_idx < len(corpus):\n",
    "        window_size = random.randint(1, max_window_size)\n",
    "        positive_word = corpus[center_word_idx]\n",
    "\n",
    "        context_word_range = (max(0, center_word_idx - window_size), min(len(corpus) - 1, center_word_idx + window_size))\n",
    "        context_word_candidates = [corpus[idx] for idx in range(context_word_range[0], context_word_range[1]+1) if idx != center_word_idx]\n",
    "\n",
    "        for context_word in context_word_candidates:\n",
    "            dataset.append((context_word, positive_word, 1))\n",
    "\n",
    "            #开始负采样\n",
    "            i = 0\n",
    "            while i < negative_sample_num:\n",
    "                negative_word_candidate = random.randint(0, vocab_size-1)\n",
    "\n",
    "                if negative_word_candidate is not positive_word:\n",
    "                    dataset.append((context_word, negative_word_candidate, 0))\n",
    "                    i += 1\n",
    "        \n",
    "        center_word_idx = min(len(corpus) - 1, center_word_idx + window_size)\n",
    "        if center_word_idx == (len(corpus) - 1):\n",
    "            center_word_idx += 1\n",
    "        if center_word_idx % 100000 == 0:\n",
    "            print(center_word_idx)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = build_data(corpus, word2id_dict, word2id_freq)\n",
    "for _, (context_word, target_word, label) in zip(range(50), dataset):\n",
    "    print(\"center_word %s, target %s, label %d\" % (id2word_dict[context_word],\n",
    "                                                   id2word_dict[target_word], label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4423d304-926a-47a5-ab21-eee2f0ba5512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(dataset, batch_size, epoch_num):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param \n",
    "    :return:\n",
    "    \"\"\"\n",
    "    center_word_batch = []\n",
    "    target_word_batch = []\n",
    "    label_batch = []\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        random.shuffle(dataset)\n",
    "        \n",
    "        for center_word, target_word, label in dataset:\n",
    "            center_word_batch.append([center_word])\n",
    "            target_word_batch.append([target_word])\n",
    "            label_batch.append(label)\n",
    "\n",
    "            if len(center_word_batch) == batch_size:\n",
    "                yield np.array(center_word_batch).astype(\"int64\"), \\\n",
    "                    np.array(target_word_batch).astype(\"int64\"), \\\n",
    "                    np.array(label_batch).astype(\"float32\")\n",
    "                center_word_batch = []\n",
    "                target_word_batch = []\n",
    "                label_batch = []\n",
    "\n",
    "    if len(center_word_batch) > 0:\n",
    "        yield np.array(center_word_batch).astype(\"int64\"), \\\n",
    "            np.array(target_word_batch).astype(\"int64\"), \\\n",
    "            np.array(label_batch).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "240aa915-a1ff-4703-ab64-c5d7f3b7cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, init_scale=0.1):\n",
    "        \"\"\"\n",
    "    \n",
    "        :param \n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            self.vocab_size, \n",
    "            self.embedding_size)\n",
    "        self.embedding.weight.data.uniform_(-0.5 / self.embedding_size, 0.5 / self.embedding_size)\n",
    "        \"\"\"\n",
    "        self.embedding_out = nn.Embedding(\n",
    "            self.vocab_size, \n",
    "            self.embedding_size)\n",
    "        self.embedding_out.weight.data.uniform_(-0.5 / self.embedding_size, 0.5 / self.embedding_size)\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, center_words, target_words, label):\n",
    "        \"\"\"\n",
    "    \n",
    "        :param \n",
    "        :return:\n",
    "        \"\"\"\n",
    "        center_words_emb = self.embedding(center_words)\n",
    "        target_words_emb = self.embedding(target_words)\n",
    "        # target_words_emb = self.embedding_out(target_words)\n",
    "\n",
    "        word_sim = torch.multiply(center_words_emb, target_words_emb)\n",
    "        word_sim = torch.sum(word_sim, axis = -1)\n",
    "        word_sim = torch.reshape(word_sim, shape=[-1])\n",
    "        pred = nn.functional.sigmoid(word_sim)\n",
    "\n",
    "        loss = nn.functional.binary_cross_entropy(nn.functional.sigmoid(word_sim), label)\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        return pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c2a68f4-d551-4279-8a02-5fdadedb3c8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m pred, loss \u001b[38;5;241m=\u001b[39m skip_gram_model(center_words_var, target_words_var, label_var)\n\u001b[0;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 17\u001b[0m \u001b[43madam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m adam\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     20\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mE:\\Program\\miniconda3\\envs\\torch222\\lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Program\\miniconda3\\envs\\torch222\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mE:\\Program\\miniconda3\\envs\\torch222\\lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mE:\\Program\\miniconda3\\envs\\torch222\\lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Program\\miniconda3\\envs\\torch222\\lib\\site-packages\\torch\\optim\\adam.py:392\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    391\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m--> 392\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[0;32m    395\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epoch_num = 1\n",
    "embedding_size = 200\n",
    "step = 0\n",
    "    \n",
    "skip_gram_model = SkipGram(vocab_size, embedding_size)\n",
    "adam = torch.optim.Adam(skip_gram_model.parameters(), lr=0.001)\n",
    "\n",
    "for center_words, target_words, label in build_batch(dataset, batch_size, epoch_num):\n",
    "    center_words_var = torch.tensor(center_words)\n",
    "    target_words_var = torch.tensor(target_words)\n",
    "    label_var = torch.tensor(label)\n",
    "\n",
    "    pred, loss = skip_gram_model(center_words_var, target_words_var, label_var)\n",
    "    \n",
    "    loss.backward()\n",
    "    adam.step()\n",
    "    adam.zero_grad()\n",
    "    \n",
    "    step += 1\n",
    "    if step % 100 == 0:\n",
    "        print(\"step %d, loss %.3f\" % (step, loss.item()))\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        embedding_matrix = skip_gram_model.embedding.weight.detach().numpy()\n",
    "        np.save(\"./embedding\", embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1081e61-eb5a-499d-ba26-3ac2be50bcd6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cos(query1_token, query2_token, embed):\n",
    "    W = embed\n",
    "    x = W[query1_token]\n",
    "    y = W[query2_token]\n",
    "    cos_sim = np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "    return cos_sim\n",
    "\n",
    "embedding_matrix = np.load(\"embedding.npy\")\n",
    "\n",
    "word2id_dict = {word: idx for word, idx in word2id_dict.items() if idx < embedding_matrix.shape[0]}\n",
    "\n",
    "word_pairs = []\n",
    "\"\"\"\n",
    "遍历embedding.npy\n",
    "找出相似度高的词输出\n",
    "最后按照相似度降序排列\n",
    "但是词语太多，全部遍历太耗费时间\n",
    "所以只输出了一部分\n",
    "\"\"\"\n",
    "for query1_token, idx1 in word2id_dict.items():\n",
    "    for query2_token, idx2 in word2id_dict.items():\n",
    "        if query1_token != query2_token:\n",
    "            cos_sim = get_cos(idx1, idx2, embedding_matrix)\n",
    "\n",
    "            if cos_sim > 0.8:\n",
    "                print(query1_token, query2_token, cos_sim)\n",
    "                word_pairs.append((query1_token, query2_token, cos_sim))\n",
    "            \n",
    "\n",
    "top_word_pairs = sorted(word_pairs, key=lambda x: x[2], reverse=True)[:50]\n",
    "\n",
    "for pair in top_word_pairs:\n",
    "    print(\"词语1：%s，词语2：%s，余弦相似度：%f\" % (pair[0], pair[1], pair[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f7d210-61d5-4a0b-8018-efa7d86ddd41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a71da1f-30ff-498f-acd4-a52f0a163fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch222",
   "language": "python",
   "name": "torch222"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
